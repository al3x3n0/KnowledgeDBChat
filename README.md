# Knowledge Database Chat Application

A comprehensive knowledge management system with LLM-based chat interface for organizational data sources. This application enables organizations to build a searchable knowledge base from multiple sources and provides an intelligent chat interface powered by local LLMs.

## ğŸŒŸ Features

### Core Functionality
- **Multi-Source Data Ingestion**: Automatically index content from GitLab, Confluence, internal websites, and document files
- **Local LLM Integration**: Privacy-focused local deployment using Ollama for complete data control
- **Semantic Search**: Advanced vector-based document retrieval using Qdrant (default) or ChromaDB
- **RAG Pipeline**: Retrieval-Augmented Generation for contextually accurate responses
- **Real-time Chat**: WebSocket-based chat interface with typing indicators
- **Document References**: Source attribution and links for all AI responses
- **LaTeX Studio**: In-app LaTeX editor with KB-assisted copilot and optional server-side PDF compilation (see `docs/LATEX_STUDIO.md`)

### Data Sources Supported
- **GitLab**: Repository files, wikis, issues, merge requests
- **Confluence**: Pages, attachments, comments
- **Web Scraping**: Internal websites and documentation
- **File Upload**: PDF, Word, Text, Markdown, HTML files
- **Extensible**: Easy to add new data source connectors

### Security & Privacy
- **Local LLM Deployment**: No data sent to external services
- **User Authentication**: JWT-based authentication with role management
- **Access Control**: Document-level permissions and user roles
- **Audit Logging**: Complete audit trail of all interactions

### MCP (Model Context Protocol)
- Exposes an MCP-compatible tool API for external agents (see `backend/app/mcp/server.py`)
- Tools include semantic search, document browsing, chat/Q&A, and `web_scrape` for extracting readable text/links from wiki/portal pages

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Sources  â”‚    â”‚    Frontend     â”‚    â”‚    Backend      â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ GitLab        â”‚    â”‚ â€¢ React/TS      â”‚    â”‚ â€¢ FastAPI       â”‚
â”‚ â€¢ Confluence    â”‚    â”‚ â€¢ Real-time UI  â”‚    â”‚ â€¢ Python        â”‚
â”‚ â€¢ Web Content   â”‚    â”‚ â€¢ Chat Interfaceâ”‚    â”‚ â€¢ LLM Interface â”‚
â”‚ â€¢ Documents     â”‚    â”‚ â€¢ Document View â”‚    â”‚ â€¢ RAG Pipeline  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data Ingestion â”‚    â”‚   Web Server    â”‚    â”‚  Vector Store   â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ Content Sync  â”‚â—„â”€â”€â–ºâ”‚ â€¢ API Routes    â”‚â—„â”€â”€â–ºâ”‚ â€¢ Qdrant        â”‚
â”‚ â€¢ Text Extract  â”‚    â”‚ â€¢ WebSocket     â”‚    â”‚ â€¢ Embeddings    â”‚
â”‚ â€¢ Processing    â”‚    â”‚ â€¢ Authenticationâ”‚    â”‚ â€¢ Similarity    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PostgreSQL    â”‚    â”‚   Local LLM     â”‚    â”‚   Monitoring    â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ Documents     â”‚    â”‚ â€¢ Ollama        â”‚    â”‚ â€¢ Health Checks â”‚
â”‚ â€¢ Chat History  â”‚    â”‚ â€¢ Multiple      â”‚    â”‚ â€¢ Metrics       â”‚
â”‚ â€¢ User Data     â”‚    â”‚   Models        â”‚    â”‚ â€¢ Logging       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Option 1: Docker Setup (Recommended)

1. **Clone and Setup**
   ```bash
   git clone <repository-url>
   cd KnowledgeDBChat
   ./setup.sh
   ```

2. **Configure Environment**
   ```bash
   # Edit backend/.env with your settings
   nano backend/.env
   ```

3. **Start Services**
   ```bash
   make start
   # or: docker compose up -d
   ```

   Optional:
   - Enable Docker-based custom tools (unsafe): `docker compose -f docker-compose.yml -f docker-compose.docker-tools.yml up -d`

4. **Access Application**
   - Frontend: http://localhost:3000
   - Backend API: http://localhost:8000
   - API Documentation: http://localhost:8000/docs

### Option 2: Manual Setup

#### Prerequisites
- Python 3.9+
- Node.js 18+
- PostgreSQL 13+
- Redis 6+
- Ollama (for LLM)

#### Backend Setup
```bash
cd backend
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
cp env.example .env
# Edit .env with your database and service URLs
uvicorn main:app --reload
```

#### Frontend Setup
```bash
cd frontend
npm install
npm start
```

#### Database Setup
```bash
# Create PostgreSQL database
createdb knowledge_db

# Run database migrations
cd backend
python -c "
import asyncio
from app.core.database import create_tables
asyncio.run(create_tables())
"
```

#### LLM Setup
```bash
# Install and start Ollama
curl -fsSL https://ollama.ai/install.sh | sh
ollama serve

# Pull a model (in another terminal)
ollama pull llama3.2:1b
```

##### Optional: Use DeepSeek (External API)
- Set backend to use DeepSeek by editing `backend/.env`:
  - `LLM_PROVIDER=deepseek`
  - `DEEPSEEK_API_KEY=...` (required)
  - Optionally adjust `DEEPSEEK_MODEL` (e.g., `deepseek-chat`) and `DEEPSEEK_MAX_RESPONSE_TOKENS`.
- Note: This sends prompts and context to an external provider. Ensure compliance with your data policies.

###### Heavy Summarization & Chunking
- Large documents are summarized in chunks and then combined into a cohesive summary.
- Heavy jobs (based on `SUMMARIZATION_HEAVY_THRESHOLD_CHARS`) automatically prefer DeepSeek if a key is configured.
- Tuning variables in `backend/.env`:
  - `SUMMARIZATION_CHUNK_SIZE_CHARS` and `SUMMARIZATION_CHUNK_OVERLAP_CHARS` control chunking.
  - `SUMMARIZATION_HEAVY_THRESHOLD_CHARS` controls when external routing is preferred.

## ğŸ“Š Configuration

### Environment Variables

#### Backend Configuration (`backend/.env`)
```bash
# Database
DATABASE_URL=postgresql://user:password@localhost:5432/knowledge_db
REDIS_URL=redis://localhost:6379/0

# LLM Configuration
LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
DEFAULT_MODEL=llama3.2:1b
EMBEDDING_MODEL=all-MiniLM-L6-v2

# Vector store
VECTOR_STORE_PROVIDER=qdrant  # qdrant | chroma
QDRANT_URL=http://localhost:6333
QDRANT_COLLECTION_NAME=knowledge_base

# DeepSeek (only if LLM_PROVIDER=deepseek)
DEEPSEEK_API_BASE=https://api.deepseek.com/v1
DEEPSEEK_API_KEY=your-deepseek-api-key
DEEPSEEK_MODEL=deepseek-chat
DEEPSEEK_TIMEOUT_SECONDS=120
DEEPSEEK_MAX_RESPONSE_TOKENS=2000

# Summarization
SUMMARIZATION_HEAVY_THRESHOLD_CHARS=30000
SUMMARIZATION_CHUNK_SIZE_CHARS=12000
SUMMARIZATION_CHUNK_OVERLAP_CHARS=800

# Security
SECRET_KEY=your-secret-key-here
ACCESS_TOKEN_EXPIRE_MINUTES=30

# Custom tools (safety)
CUSTOM_TOOL_DOCKER_ENABLED=false

# Agent governance (writes)
AGENT_KB_PATCH_APPLY_ENABLED=false

# Data Sources
GITLAB_URL=https://gitlab.company.com
GITLAB_TOKEN=your-gitlab-token
CONFLUENCE_URL=https://company.atlassian.net
CONFLUENCE_USER=your-username
CONFLUENCE_API_TOKEN=your-api-token
```

### Tool Policies (Allow-By-Default)

The platform supports a unified tool policy layer across agents, MCP, and workflows:
- Default behavior is allow-by-default; add explicit denies to block tools.
- Policies can also mark a tool as `require_approval`, which creates a pending approval in the tool audit log.

Conventions:
- MCP tool names are policy-addressable as `mcp:<tool_name>` (example: `mcp:web_scrape`, `mcp:create_repo_report`)
- Custom user tools are policy-addressable as `user_tool:<uuid>` (and you can match all with `user_tool:*`)

Constraints (optional, stored in `constraints` JSON):
- `allowed_domains`: list of allowed hostnames/suffixes for `url`/`repo_url` args
- `deny_private_networks`: boolean; blocks `localhost`, private IPs, `.local`, `.internal`
- `max_cost_tier`: `"low"|"medium"|"high"`

Endpoints:
- `GET /api/v1/tools/registry` (includes built-in + your custom tools; shows `allowed` / `require_approval`)
- `POST /api/v1/tools/evaluate` (debug: evaluate a tool call against current policies)
- `GET/POST/DELETE /api/v1/tools/policies` (your per-user policy rules)
- `GET/POST/DELETE /api/v1/admin/tool-policies` (admin policy rules)
- `GET /api/v1/audit/tools` and `POST /api/v1/audit/tools/{audit_id}/approve|reject|run` (owner or admin)

Approvals:
- Tools marked `require_approval` use a dual-approval model by default: resource owner + admin must approve before `run`.

Bootstrap (optional):
- Seed a recommended baseline (approval gates on network/write tools):
  - `python3 scripts/bootstrap_tool_policies.py --dry-run`
  - `python3 scripts/bootstrap_tool_policies.py`
- Optionally restrict network tools to specific domains:
  - `python3 scripts/bootstrap_tool_policies.py --allowed-domains wiki.company.com,github.com,gitlab.company.com`

### Data Source Configuration

#### GitLab Integration
1. Create a Personal Access Token in GitLab
2. Add to environment variables
3. Configure repositories in the admin panel

#### GitHub Integration
- Create a Personal Access Token in GitHub with repo read permissions.
- In Admin â†’ Data Sources, create a source with `source_type: "github"` and config like:
  {
    "token": "ghp_...",
    "repos": ["owner/repo1", {"owner": "org", "repo": "repo2"}],
    "include_files": true,
    "include_issues": true,
    "include_pull_requests": false,
    "include_wiki": false,
    "file_extensions": [".md", ".txt", ".py"]
  }
- Start sync from the Admin panel to index content.

- Optional keys:
  - `ignore_globs`: glob patterns to exclude paths (e.g., ["**/node_modules/**", "**/dist/**"]).
  - `incremental_files` (default true): only fetch files changed since last sync.
  - `max_pages` (default 10): pagination cap for issues/commits.
  - `use_gitignore` (default false): auto-merge root .gitignore patterns into filters.

### Source Scheduling
- In each source's `config`, you can set:
  - `auto_sync`: boolean â€” enable automatic syncs.
  - `sync_interval_minutes`: number â€” run at this interval (e.g., 60 for hourly).
  - `cron`: string â€” optional cron expression (e.g., `0 2 * * *` for 2 AM daily). If present, it supersedes interval. Validated via croniter.
  - `sync_only_changed`: boolean â€” if false, scheduled runs force a full sync (heavy).

Admin UI exposes toggles for Auto Sync and Sync Only Changed, an Interval field, and shows ETAs during runs. You can also run a Dry Run preview and Cancel ongoing syncs.

#### Confluence Integration
1. Create an API token in Atlassian
2. Add credentials to environment variables
3. Configure spaces in the admin panel

## ğŸ”§ Development

### Project Structure
```
KnowledgeDBChat/
â”œâ”€â”€ backend/                 # Python FastAPI backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/            # API routes and endpoints
â”‚   â”‚   â”‚   â””â”€â”€ endpoints/  # Individual endpoint modules
â”‚   â”‚   â”œâ”€â”€ core/           # Core functionality (config, database)
â”‚   â”‚   â”œâ”€â”€ models/         # SQLAlchemy database models
â”‚   â”‚   â”œâ”€â”€ schemas/        # Pydantic request/response models
â”‚   â”‚   â”œâ”€â”€ services/       # Business logic services
â”‚   â”‚   â””â”€â”€ utils/          # Utility functions
â”‚   â”œâ”€â”€ requirements.txt    # Python dependencies
â”‚   â”œâ”€â”€ Dockerfile         # Docker configuration
â”‚   â””â”€â”€ main.py            # Application entry point
â”œâ”€â”€ frontend/               # React TypeScript frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/     # React components
â”‚   â”‚   â”‚   â”œâ”€â”€ chat/      # Chat-related components
â”‚   â”‚   â”‚   â”œâ”€â”€ documents/ # Document management
â”‚   â”‚   â”‚   â””â”€â”€ common/    # Shared components
â”‚   â”‚   â”œâ”€â”€ contexts/       # React contexts
â”‚   â”‚   â”œâ”€â”€ hooks/          # Custom React hooks
â”‚   â”‚   â”œâ”€â”€ services/       # API service functions
â”‚   â”‚   â”œâ”€â”€ types/          # TypeScript type definitions
â”‚   â”‚   â””â”€â”€ utils/          # Utility functions
â”‚   â”œâ”€â”€ package.json       # Node.js dependencies
â”‚   â””â”€â”€ Dockerfile         # Docker configuration
â”œâ”€â”€ data/                   # Data storage directories
â”‚   â”œâ”€â”€ documents/         # Uploaded documents
â”‚   â”œâ”€â”€ chroma_db/         # Vector database
â”‚   â””â”€â”€ logs/              # Application logs
â”œâ”€â”€ docker-compose.yml      # Multi-service Docker setup
â”œâ”€â”€ setup.sh               # Automated setup script
â””â”€â”€ README.md
```

### API Endpoints

#### Authentication
- `POST /api/v1/auth/register` - User registration
- `POST /api/v1/auth/login` - User login
- `GET /api/v1/auth/me` - Get current user

#### Chat
- `POST /api/v1/chat/sessions` - Create chat session
- `GET /api/v1/chat/sessions` - List user sessions
- `POST /api/v1/chat/sessions/{id}/messages` - Send message
- `WS /api/v1/chat/sessions/{id}/ws` - WebSocket chat

#### Agents
- `POST /api/v1/agent/chat` - Agentic chat (tool calling + routing)
- `GET /api/v1/agent/tools` - List available agent tools
- `GET /api/v1/agent/capabilities` - List routing capabilities
- `GET /api/v1/agent/agents?search=...` - List agent definitions (admin UI)
- `POST /api/v1/agent/agents` - Create agent definition (admin only)
- `PUT /api/v1/agent/agents/{id}` - Update agent definition (admin only)
- `DELETE /api/v1/agent/agents/{id}` - Delete agent definition (admin only)
- `POST /api/v1/agent/agents/{id}/duplicate` - Duplicate agent definition (admin only)

#### Documents
- `GET /api/v1/documents/` - List documents
- `POST /api/v1/documents/upload` - Upload document
- `DELETE /api/v1/documents/{id}` - Delete document
- `POST /api/v1/documents/{id}/reprocess` - Reprocess document

#### Admin
- `GET /api/v1/documents/sources/` - List data sources
- `POST /api/v1/documents/sources/` - Create data source
- `POST /api/v1/documents/sources/{id}/sync` - Trigger sync

### Development Commands

```bash
# Backend development
cd backend
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
uvicorn main:app --reload --host 0.0.0.0 --port 8000

# Frontend development
cd frontend
npm install
npm start

# Database operations
# Create migration
alembic revision --autogenerate -m "description"

# Run migrations
alembic upgrade head

# Reset database
python -c "
import asyncio
from app.core.database import drop_tables, create_tables
asyncio.run(drop_tables())
asyncio.run(create_tables())
"
```

## ğŸ§ª Testing

```bash
# Backend tests
cd backend
pytest

# Frontend tests
cd frontend
npm run test:ci

# Integration tests
docker compose -f docker-compose.test.yml up --abort-on-container-exit
```

## ğŸ“¦ Production Deployment

### Docker Production Setup
```bash
# Build production images
docker compose -f docker-compose.prod.yml build

# Deploy with environment-specific config
docker compose -f docker-compose.prod.yml up -d
```

### Manual Production Setup
1. Set up production database (PostgreSQL)
2. Configure Redis instance
3. Set up Ollama with required models
4. Deploy backend with gunicorn
5. Build and serve frontend with nginx
6. Configure reverse proxy and SSL

## ğŸ” Monitoring & Maintenance

### Health Checks
- Backend `/health` - Application health status
- Backend `/api/v1/health` - Detailed service health
- Nginx `/health` (http://localhost:3000/health) - Frontend reverse proxy health
- MinIO live check (http://localhost:9000/minio/health/live)

### Logs
```bash
# View application logs
docker compose logs -f backend

# View specific service logs
docker compose logs -f ollama
```

### Backup
```bash
# Database backup
pg_dump knowledge_db > backup.sql

# Vector database backup
tar -czf chroma_backup.tar.gz data/chroma_db/
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Make your changes and add tests
4. Commit: `git commit -am 'Add feature'`
5. Push: `git push origin feature-name`
6. Create a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ†˜ Support & Troubleshooting

### Common Issues

**Ollama Connection Failed**
- Ensure Ollama is running: `ollama serve`
- Check model is available: `ollama list`
- Verify OLLAMA_BASE_URL in configuration

**Vector Search Not Working**
- Check vector store is running (Qdrant container) or Chroma directory permissions (if using Chroma)
- Verify embedding model is downloaded
- Restart backend/celery and re-ingest documents if needed

**Database Connection Issues**
- Verify PostgreSQL is running
- Check DATABASE_URL configuration
- Ensure database exists and is accessible

### Getting Help
1. Check the [Documentation](docs/)
2. Search [Issues](../../issues)
3. Create a new issue with detailed information
4. Join our [Discord Community](discord-invite-link)

---

**Built with â¤ï¸ for organizational knowledge management**
