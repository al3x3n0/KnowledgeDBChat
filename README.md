# Knowledge Database Chat Application

A comprehensive knowledge management system with LLM-based chat interface for organizational data sources. This application enables organizations to build a searchable knowledge base from multiple sources and provides an intelligent chat interface powered by local LLMs.

## ğŸŒŸ Features

### Core Functionality
- **Multi-Source Data Ingestion**: Automatically index content from GitLab, Confluence, internal websites, and document files
- **Local LLM Integration**: Privacy-focused local deployment using Ollama for complete data control
- **Semantic Search**: Advanced vector-based document retrieval using ChromaDB
- **RAG Pipeline**: Retrieval-Augmented Generation for contextually accurate responses
- **Real-time Chat**: WebSocket-based chat interface with typing indicators
- **Document References**: Source attribution and links for all AI responses

### Data Sources Supported
- **GitLab**: Repository files, wikis, issues, merge requests
- **Confluence**: Pages, attachments, comments
- **Web Scraping**: Internal websites and documentation
- **File Upload**: PDF, Word, Text, Markdown, HTML files
- **Extensible**: Easy to add new data source connectors

### Security & Privacy
- **Local LLM Deployment**: No data sent to external services
- **User Authentication**: JWT-based authentication with role management
- **Access Control**: Document-level permissions and user roles
- **Audit Logging**: Complete audit trail of all interactions

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Sources  â”‚    â”‚    Frontend     â”‚    â”‚    Backend      â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ GitLab        â”‚    â”‚ â€¢ React/TS      â”‚    â”‚ â€¢ FastAPI       â”‚
â”‚ â€¢ Confluence    â”‚    â”‚ â€¢ Real-time UI  â”‚    â”‚ â€¢ Python        â”‚
â”‚ â€¢ Web Content   â”‚    â”‚ â€¢ Chat Interfaceâ”‚    â”‚ â€¢ LLM Interface â”‚
â”‚ â€¢ Documents     â”‚    â”‚ â€¢ Document View â”‚    â”‚ â€¢ RAG Pipeline  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data Ingestion â”‚    â”‚   Web Server    â”‚    â”‚  Vector Store   â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ Content Sync  â”‚â—„â”€â”€â–ºâ”‚ â€¢ API Routes    â”‚â—„â”€â”€â–ºâ”‚ â€¢ ChromaDB      â”‚
â”‚ â€¢ Text Extract  â”‚    â”‚ â€¢ WebSocket     â”‚    â”‚ â€¢ Embeddings    â”‚
â”‚ â€¢ Processing    â”‚    â”‚ â€¢ Authenticationâ”‚    â”‚ â€¢ Similarity    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PostgreSQL    â”‚    â”‚   Local LLM     â”‚    â”‚   Monitoring    â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ Documents     â”‚    â”‚ â€¢ Ollama        â”‚    â”‚ â€¢ Health Checks â”‚
â”‚ â€¢ Chat History  â”‚    â”‚ â€¢ Multiple      â”‚    â”‚ â€¢ Metrics       â”‚
â”‚ â€¢ User Data     â”‚    â”‚   Models        â”‚    â”‚ â€¢ Logging       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Option 1: Docker Setup (Recommended)

1. **Clone and Setup**
   ```bash
   git clone <repository-url>
   cd KnowledgeDBChat
   ./setup.sh
   ```

2. **Configure Environment**
   ```bash
   # Edit backend/.env with your settings
   nano backend/.env
   ```

3. **Start Services**
   ```bash
   docker-compose up -d
   ```

4. **Access Application**
   - Frontend: http://localhost:3000
   - Backend API: http://localhost:8000
   - API Documentation: http://localhost:8000/docs

### Option 2: Manual Setup

#### Prerequisites
- Python 3.9+
- Node.js 18+
- PostgreSQL 13+
- Redis 6+
- Ollama (for LLM)

#### Backend Setup
```bash
cd backend
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
cp env.example .env
# Edit .env with your database and service URLs
uvicorn main:app --reload
```

#### Frontend Setup
```bash
cd frontend
npm install
npm start
```

#### Database Setup
```bash
# Create PostgreSQL database
createdb knowledge_db

# Run database migrations
cd backend
python -c "
import asyncio
from app.core.database import create_tables
asyncio.run(create_tables())
"
```

#### LLM Setup
```bash
# Install and start Ollama
curl -fsSL https://ollama.ai/install.sh | sh
ollama serve

# Pull a model (in another terminal)
ollama pull llama2
```

## ğŸ“Š Configuration

### Environment Variables

#### Backend Configuration (`backend/.env`)
```bash
# Database
DATABASE_URL=postgresql://user:password@localhost:5432/knowledge_db
REDIS_URL=redis://localhost:6379/0

# LLM Configuration
OLLAMA_BASE_URL=http://localhost:11434
DEFAULT_MODEL=llama2
EMBEDDING_MODEL=all-MiniLM-L6-v2

# Security
SECRET_KEY=your-secret-key-here
ACCESS_TOKEN_EXPIRE_MINUTES=30

# Data Sources
GITLAB_URL=https://gitlab.company.com
GITLAB_TOKEN=your-gitlab-token
CONFLUENCE_URL=https://company.atlassian.net
CONFLUENCE_USER=your-username
CONFLUENCE_API_TOKEN=your-api-token
```

### Data Source Configuration

#### GitLab Integration
1. Create a Personal Access Token in GitLab
2. Add to environment variables
3. Configure repositories in the admin panel

#### Confluence Integration
1. Create an API token in Atlassian
2. Add credentials to environment variables
3. Configure spaces in the admin panel

## ğŸ”§ Development

### Project Structure
```
KnowledgeDBChat/
â”œâ”€â”€ backend/                 # Python FastAPI backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/            # API routes and endpoints
â”‚   â”‚   â”‚   â””â”€â”€ endpoints/  # Individual endpoint modules
â”‚   â”‚   â”œâ”€â”€ core/           # Core functionality (config, database)
â”‚   â”‚   â”œâ”€â”€ models/         # SQLAlchemy database models
â”‚   â”‚   â”œâ”€â”€ schemas/        # Pydantic request/response models
â”‚   â”‚   â”œâ”€â”€ services/       # Business logic services
â”‚   â”‚   â””â”€â”€ utils/          # Utility functions
â”‚   â”œâ”€â”€ requirements.txt    # Python dependencies
â”‚   â”œâ”€â”€ Dockerfile         # Docker configuration
â”‚   â””â”€â”€ main.py            # Application entry point
â”œâ”€â”€ frontend/               # React TypeScript frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/     # React components
â”‚   â”‚   â”‚   â”œâ”€â”€ chat/      # Chat-related components
â”‚   â”‚   â”‚   â”œâ”€â”€ documents/ # Document management
â”‚   â”‚   â”‚   â””â”€â”€ common/    # Shared components
â”‚   â”‚   â”œâ”€â”€ contexts/       # React contexts
â”‚   â”‚   â”œâ”€â”€ hooks/          # Custom React hooks
â”‚   â”‚   â”œâ”€â”€ services/       # API service functions
â”‚   â”‚   â”œâ”€â”€ types/          # TypeScript type definitions
â”‚   â”‚   â””â”€â”€ utils/          # Utility functions
â”‚   â”œâ”€â”€ package.json       # Node.js dependencies
â”‚   â””â”€â”€ Dockerfile         # Docker configuration
â”œâ”€â”€ data/                   # Data storage directories
â”‚   â”œâ”€â”€ documents/         # Uploaded documents
â”‚   â”œâ”€â”€ chroma_db/         # Vector database
â”‚   â””â”€â”€ logs/              # Application logs
â”œâ”€â”€ docker-compose.yml      # Multi-service Docker setup
â”œâ”€â”€ setup.sh               # Automated setup script
â””â”€â”€ README.md
```

### API Endpoints

#### Authentication
- `POST /api/v1/auth/register` - User registration
- `POST /api/v1/auth/login` - User login
- `GET /api/v1/auth/me` - Get current user

#### Chat
- `POST /api/v1/chat/sessions` - Create chat session
- `GET /api/v1/chat/sessions` - List user sessions
- `POST /api/v1/chat/sessions/{id}/messages` - Send message
- `WS /api/v1/chat/sessions/{id}/ws` - WebSocket chat

#### Documents
- `GET /api/v1/documents/` - List documents
- `POST /api/v1/documents/upload` - Upload document
- `DELETE /api/v1/documents/{id}` - Delete document
- `POST /api/v1/documents/{id}/reprocess` - Reprocess document

#### Admin
- `GET /api/v1/documents/sources/` - List data sources
- `POST /api/v1/documents/sources/` - Create data source
- `POST /api/v1/documents/sources/{id}/sync` - Trigger sync

### Development Commands

```bash
# Backend development
cd backend
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
uvicorn main:app --reload --host 0.0.0.0 --port 8000

# Frontend development
cd frontend
npm install
npm start

# Database operations
# Create migration
alembic revision --autogenerate -m "description"

# Run migrations
alembic upgrade head

# Reset database
python -c "
import asyncio
from app.core.database import drop_tables, create_tables
asyncio.run(drop_tables())
asyncio.run(create_tables())
"
```

## ğŸ§ª Testing

```bash
# Backend tests
cd backend
pytest

# Frontend tests
cd frontend
npm test

# Integration tests
docker-compose -f docker-compose.test.yml up --abort-on-container-exit
```

## ğŸ“¦ Production Deployment

### Docker Production Setup
```bash
# Build production images
docker-compose -f docker-compose.prod.yml build

# Deploy with environment-specific config
docker-compose -f docker-compose.prod.yml up -d
```

### Manual Production Setup
1. Set up production database (PostgreSQL)
2. Configure Redis instance
3. Set up Ollama with required models
4. Deploy backend with gunicorn
5. Build and serve frontend with nginx
6. Configure reverse proxy and SSL

## ğŸ” Monitoring & Maintenance

### Health Checks
- `/health` - Application health status
- `/api/v1/health` - Detailed service health

### Logs
```bash
# View application logs
docker-compose logs -f backend

# View specific service logs
docker-compose logs -f ollama
```

### Backup
```bash
# Database backup
pg_dump knowledge_db > backup.sql

# Vector database backup
tar -czf chroma_backup.tar.gz data/chroma_db/
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Make your changes and add tests
4. Commit: `git commit -am 'Add feature'`
5. Push: `git push origin feature-name`
6. Create a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ†˜ Support & Troubleshooting

### Common Issues

**Ollama Connection Failed**
- Ensure Ollama is running: `ollama serve`
- Check model is available: `ollama list`
- Verify OLLAMA_BASE_URL in configuration

**Vector Search Not Working**
- Check ChromaDB directory permissions
- Verify embedding model is downloaded
- Restart vector store service

**Database Connection Issues**
- Verify PostgreSQL is running
- Check DATABASE_URL configuration
- Ensure database exists and is accessible

### Getting Help
1. Check the [Documentation](docs/)
2. Search [Issues](../../issues)
3. Create a new issue with detailed information
4. Join our [Discord Community](discord-invite-link)

---

**Built with â¤ï¸ for organizational knowledge management**
