services:
  # PostgreSQL Database
  postgres:
    image: postgres:15-alpine
    container_name: knowledge_db_postgres
    environment:
      POSTGRES_DB: knowledge_db
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./data/postgres-init:/docker-entrypoint-initdb.d
    networks:
      - knowledge_net

  # Redis for caching and task queue
  redis:
    image: redis:7-alpine
    container_name: knowledge_db_redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    networks:
      - knowledge_net

  # Qdrant Vector Database (default; set VECTOR_STORE_PROVIDER=chroma to use embedded ChromaDB)
  qdrant:
    image: qdrant/qdrant:latest
    container_name: knowledge_db_qdrant
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage
    networks:
      - knowledge_net

  # MinIO for object storage
  minio:
    image: minio/minio:latest
    container_name: knowledge_db_minio
    ports:
      - "9000:9000"  # API
      - "9001:9001"  # Console
    volumes:
      - minio_data:/data
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin
    command: server /data --console-address ":9001"
    networks:
      - knowledge_net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # Backend API
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    image: knowledge_db_backend:latest  # Tag image so celery can reuse it
    container_name: knowledge_db_backend
    environment:
      - DATABASE_URL=postgresql://user:password@postgres:5432/knowledge_db
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
      - OLLAMA_BASE_URL=http://ollama:11434
      - DEFAULT_MODEL=llama3.2:1b  # Force smallest model for Mac
      - CHROMA_PERSIST_DIRECTORY=/app/data/chroma_db
      # Vector store: "qdrant" (default) or "chroma"
      - VECTOR_STORE_PROVIDER=qdrant
      - QDRANT_URL=http://qdrant:6333
      - QDRANT_COLLECTION_NAME=knowledge_base
      - MINIO_ENDPOINT=minio:9000
      - MINIO_ACCESS_KEY=minioadmin
      - MINIO_SECRET_KEY=minioadmin
      - MINIO_BUCKET_NAME=documents
      - MINIO_USE_SSL=false
      - MINIO_PRESIGNED_URL_EXPIRY=3600
      - MINIO_PROXY_BASE_URL=http://localhost:3000/minio
      - CHROMA_TELEMETRY_ENABLED=false
      - SECRET_KEY=your-secret-key-here  # Must match JWT_SECRET in video-streamer
      # Backpressure / pool tuning for dev stability (avoid exhausting Postgres connections)
      - DB_POOL_SIZE=10
      - DB_MAX_OVERFLOW=10
      - DB_POOL_TIMEOUT_SECONDS=5
      - DB_SESSION_ACQUIRE_TIMEOUT_SECONDS=2
      # Kroki for local Mermaid diagram rendering
      - KROKI_URL=http://kroki:8000
      # Transcription configuration
      - WHISPER_MODEL_SIZE=medium
      - WHISPER_DEVICE=auto
      - TRANSCRIPTION_LANGUAGE=ru
      # Don't block API startup on large model downloads; preload can be enabled when needed.
      - PRELOAD_WHISPER_MODEL=false
      # Run migrations in the background so the API can start serving immediately.
      - RUN_ALEMBIC_MIGRATIONS=true
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data
      - ./backend:/app
      # Mount Whisper model cache directory
      - whisper_models:/root/.cache/knowledge_db_transcriber
    depends_on:
      - postgres
      - redis
      - qdrant
      - minio
    networks:
      - knowledge_net
    command: uvicorn main:app --host 0.0.0.0 --port 8000 --reload --reload-dir app --reload-include "main.py"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Video Streaming Service (Go microservice)
  video-streamer:
    build:
      context: ./video-streamer
      dockerfile: Dockerfile
    container_name: knowledge_db_video_streamer
    environment:
      - MINIO_ENDPOINT=minio:9000
      - MINIO_ACCESS_KEY=minioadmin
      - MINIO_SECRET_KEY=minioadmin
      - MINIO_BUCKET_NAME=documents
      - MINIO_USE_SSL=false
      - JWT_SECRET=your-secret-key-here # Must match backend SECRET_KEY
      - BACKEND_API_URL=http://backend:8000/api/v1/documents
      - PORT=8080
      - CORS_ORIGIN=*
      - DATABASE_URL=postgresql://user:password@postgres:5432/knowledge_db
    ports:
      - "8080:8080"
    depends_on:
      - minio
      - postgres
    networks:
      - knowledge_net
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # Kroki for local diagram rendering (Mermaid, PlantUML, etc.)
  kroki:
    image: yuzutech/kroki:latest
    container_name: knowledge_db_kroki
    ports:
      - "8001:8000"
    networks:
      - knowledge_net
    environment:
      - KROKI_MAX_URI_LENGTH=8000
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # Ollama for local LLM
  ollama:
    image: ollama/ollama:latest
    container_name: knowledge_db_ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - knowledge_net
    environment:
      - OLLAMA_HOST=0.0.0.0
      # Force CPU-only mode for Mac compatibility (disable GPU completely)
      - OLLAMA_NUM_GPU=0
      - CUDA_VISIBLE_DEVICES=""
      # Limit memory usage (adjust based on your Mac's available RAM)
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_NUM_PARALLEL=1
      # Additional memory management
      - OLLAMA_KEEP_ALIVE=2m  # Shorter keep-alive to free memory faster
      - OLLAMA_NUM_THREAD=2  # Reduce threads to save memory
      # Force CPU-only (disable Metal/GPU on Mac)
      - OLLAMA_GPU_LAYERS=0
      - METAL_DEVICE=0
    # Resource limits for Mac (reduced for 8GB Docker Desktop)
    deploy:
      resources:
        limits:
          # Reduced to 4GB since Docker Desktop only has ~7.6GB total
          # This leaves memory for other containers (postgres, redis, backend)
          memory: 4G
          cpus: '4.0'  # Limit CPU cores
        reservations:
          memory: 1G  # Reduced reservation
          cpus: '1.0'
    # Increase shared memory for model loading (important for Mac)
    shm_size: 2gb
    # Explicitly disable GPU access
    devices: []
    # Restart policy
    restart: unless-stopped

  # Frontend (React)
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.dev
    container_name: knowledge_db_frontend
    # Don't expose port directly - nginx will proxy to it
    expose:
      - "3000"
    volumes:
      - ./frontend:/app
      - /app/node_modules
    environment:
      - REACT_APP_API_URL=http://localhost:3000
      - REACT_APP_WS_URL=ws://localhost:3000
    depends_on:
      - backend
    networks:
      - knowledge_net
    command: npm start

  # Nginx reverse proxy (frontend, API, and MinIO)
  nginx:
    image: nginx:alpine
    container_name: knowledge_db_nginx
    ports:
      - "3000:80"
    volumes:
      - ./frontend/nginx.conf:/etc/nginx/conf.d/default.conf:ro
    depends_on:
      - backend
      - minio
      - frontend
    networks:
      - knowledge_net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # Celery worker for background tasks
  celery:
    build:
      context: ./backend
      dockerfile: Dockerfile
    image: knowledge_db_backend:latest  # Share image with backend to avoid duplicate builds
    container_name: knowledge_db_celery
    environment:
      - DATABASE_URL=postgresql://user:password@postgres:5432/knowledge_db
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
      # Vector store: "qdrant" (default) or "chroma"
      - VECTOR_STORE_PROVIDER=qdrant
      - QDRANT_URL=http://qdrant:6333
      - QDRANT_COLLECTION_NAME=knowledge_base
      - MINIO_ENDPOINT=minio:9000
      - MINIO_ACCESS_KEY=minioadmin
      - MINIO_SECRET_KEY=minioadmin
      - MINIO_BUCKET_NAME=documents
      - MINIO_USE_SSL=false
      - MINIO_PRESIGNED_URL_EXPIRY=3600
      - MINIO_PROXY_BASE_URL=http://localhost:3000/minio
      - CHROMA_TELEMETRY_ENABLED=false
      # Kroki for local Mermaid diagram rendering
      - KROKI_URL=http://kroki:8000
      # Transcription configuration
      - WHISPER_MODEL_SIZE=base
      - WHISPER_DEVICE=auto
      - TRANSCRIPTION_LANGUAGE=ru
      # Avoid blocking worker startup on model downloads; enable if you want warm transcriptions.
      - PRELOAD_WHISPER_MODEL=false
      # Only the API service should run migrations.
      - RUN_ALEMBIC_MIGRATIONS=false
      # Prevent OpenBLAS hangs in forked Celery workers while allowing parallelism
      - OPENBLAS_MAIN_FREE=1
      - GOTO_NUM_THREADS=4
      - OMP_NUM_THREADS=4
      - MKL_NUM_THREADS=4
    volumes:
      - ./data:/app/data
      - ./backend:/app
      # Mount Whisper model cache directory (shared with backend)
      - whisper_models:/root/.cache/knowledge_db_transcriber
    depends_on:
      - postgres
      - redis
      - qdrant
      - minio
      - kroki
    networks:
      - knowledge_net
    command: celery -A app.core.celery worker --loglevel=info
    # Increase container memory (compose-only) and shared memory to handle larger models
    mem_limit: 6g
    shm_size: 2gb
    # Optional Swarm-style limits (ignored by plain compose but useful if deployed via Swarm)
    deploy:
      resources:
        limits:
          memory: 6G
        reservations:
          memory: 2G
    healthcheck:
      test: ["CMD", "celery", "-A", "app.core.celery", "inspect", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Dedicated Celery worker for LaTeX compilation (recommended to isolate heavy/unsafe workloads)
  celery_latex:
    build:
      context: ./backend
      dockerfile: Dockerfile.latex-worker
    image: knowledge_db_latex_worker:latest
    container_name: knowledge_db_celery_latex
    environment:
      - DATABASE_URL=postgresql://user:password@postgres:5432/knowledge_db
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
      - MINIO_ENDPOINT=minio:9000
      - MINIO_ACCESS_KEY=minioadmin
      - MINIO_SECRET_KEY=minioadmin
      - MINIO_BUCKET_NAME=documents
      - MINIO_USE_SSL=false
      - MINIO_PRESIGNED_URL_EXPIRY=3600
      - MINIO_PROXY_BASE_URL=http://localhost:3000/minio
      - CHROMA_TELEMETRY_ENABLED=false
      - RUN_ALEMBIC_MIGRATIONS=false
      - PYTHONDONTWRITEBYTECODE=1
      - OPENBLAS_MAIN_FREE=1
      - GOTO_NUM_THREADS=1
      - OMP_NUM_THREADS=1
      - MKL_NUM_THREADS=1
    depends_on:
      - postgres
      - redis
      - minio
    networks:
      - knowledge_net
    command: celery -A app.core.celery_latex worker -Q latex --loglevel=info --concurrency=1
    mem_limit: 2g
    shm_size: 512mb
    pids_limit: 256
    read_only: true
    tmpfs:
      - /tmp
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL

volumes:
  postgres_data:
  redis_data:
  ollama_data:
  minio_data:
  whisper_models:  # Persistent storage for Whisper models
  qdrant_data:

networks:
  knowledge_net:
    driver: bridge
